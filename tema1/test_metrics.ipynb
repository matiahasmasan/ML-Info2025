{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c520e08",
   "metadata": {},
   "source": [
    "# Testing Classification Metrics\n",
    "\n",
    "Testing our metrics implementation:\n",
    "1. 2x2 (Sick vs Healthy)\n",
    "2. 3x3 (Severe, Moderate, Mild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2ed45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our metrics module\n",
    "from metrics import (\n",
    "    evaluate_binary,\n",
    "    calculate_per_class_metrics,\n",
    "    calculate_macro_average,\n",
    "    calculate_weighted_average\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c81ef6",
   "metadata": {},
   "source": [
    "## 1. Binary Classification: Sick vs Healthy\n",
    "\n",
    "\n",
    "2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86111242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.800\n",
      "precision: 0.174\n",
      "recall: 0.800\n",
      "f1_score: 0.286\n"
     ]
    }
   ],
   "source": [
    "# Binary confusion matrix\n",
    "binary_cm = [\n",
    "    [760, 190],  # [TN, FP]\n",
    "    [10, 40]     # [FN, TP]\n",
    "]\n",
    "\n",
    "# Calculate and display all metrics\n",
    "binary_metrics = evaluate_binary(binary_cm)\n",
    "for metric_name, value in binary_metrics.items():\n",
    "    print(f\"{metric_name}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51ceef",
   "metadata": {},
   "source": [
    "## 2. Multi-class Classification: Disease Severity\n",
    "\n",
    "3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918bb0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for each severity level:\n",
      "\n",
      "Severe:\n",
      "precision: 0.758\n",
      "recall: 0.714\n",
      "f1_score: 0.735\n",
      "\n",
      "Moderate:\n",
      "precision: 0.727\n",
      "recall: 0.727\n",
      "f1_score: 0.727\n",
      "\n",
      "Mild:\n",
      "precision: 0.893\n",
      "recall: 0.909\n",
      "f1_score: 0.901\n",
      "\n",
      "Macro-averaged metrics:\n",
      "macro_precision: 0.793\n",
      "macro_recall: 0.784\n",
      "macro_f1: 0.788\n"
     ]
    }
   ],
   "source": [
    "# Multi-class confusion matrix\n",
    "multiclass_cm = [\n",
    "    [25, 8, 2],    # Severe predictions\n",
    "    [5, 40, 10],   # Moderate predictions\n",
    "    [3, 7, 100]    # Mild predictions\n",
    "]\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(\"Metrics for each severity level:\")\n",
    "for i, severity in enumerate(['Severe', 'Moderate', 'Mild']):\n",
    "    metrics = calculate_per_class_metrics(multiclass_cm, i)\n",
    "    print(f\"\\n{severity}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.3f}\")\n",
    "\n",
    "# Calculate and display macro-averages\n",
    "print(\"\\nMacro-averaged metrics:\")\n",
    "macro_metrics = calculate_macro_average(multiclass_cm)\n",
    "for metric_name, value in macro_metrics.items():\n",
    "    print(f\"{metric_name}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b22e8",
   "metadata": {},
   "source": [
    "# Testing Classification Metrics\n",
    "\n",
    "\n",
    "1. Binary Classification Metrics\n",
    "   - Accuracy, Precision, Recall, F1-score\n",
    "   \n",
    "2. Multi-class Classification Metrics\n",
    "   - Per-class metrics\n",
    "   - Macro averages\n",
    "   - Weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18b2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our metrics module\n",
    "from metrics import (\n",
    "    accuracy, precision, recall, f1,\n",
    "    evaluate_binary,\n",
    "    calculate_per_class_metrics,\n",
    "    calculate_macro_average,\n",
    "    calculate_weighted_average\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe09d5d",
   "metadata": {},
   "source": [
    "## 1. Binary Classification Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f8d7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual metrics:\n",
      "Accuracy: 0.850\n",
      "Precision: 0.778\n",
      "Recall: 0.875\n",
      "F1 Score: 0.824\n",
      "\n",
      "Using evaluate_binary function:\n",
      "accuracy: 0.850\n",
      "precision: 0.778\n",
      "recall: 0.875\n",
      "f1_score: 0.824\n"
     ]
    }
   ],
   "source": [
    "# Create a binary confusion matrix\n",
    "binary_cm = [\n",
    "    [50, 10],  # [TN, FP]\n",
    "    [5, 35]    # [FN, TP]\n",
    "]\n",
    "\n",
    "# Test individual metrics\n",
    "tp, tn = 35, 50\n",
    "fp, fn = 10, 5\n",
    "\n",
    "print(\"Individual metrics:\")\n",
    "print(f\"Accuracy: {accuracy(tp, tn, fp, fn):.3f}\")\n",
    "print(f\"Precision: {precision(tp, fp):.3f}\")\n",
    "print(f\"Recall: {recall(tp, fn):.3f}\")\n",
    "print(f\"F1 Score: {f1(precision(tp, fp), recall(tp, fn)):.3f}\")\n",
    "\n",
    "print(\"\\nUsing evaluate_binary function:\")\n",
    "metrics = evaluate_binary(binary_cm)\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24879e0f",
   "metadata": {},
   "source": [
    "## 2. Multi-class Classification Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51532d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class metrics:\n",
      "\n",
      "Class 0:\n",
      "precision: 0.714\n",
      "recall: 0.769\n",
      "f1_score: 0.741\n",
      "\n",
      "Class 1:\n",
      "precision: 0.789\n",
      "recall: 0.750\n",
      "f1_score: 0.769\n",
      "\n",
      "Class 2:\n",
      "precision: 0.800\n",
      "recall: 0.800\n",
      "f1_score: 0.800\n",
      "\n",
      "Macro-averaged metrics:\n",
      "macro_precision: 0.768\n",
      "macro_recall: 0.773\n",
      "macro_f1: 0.770\n",
      "\n",
      "Weighted-averaged metrics:\n",
      "weighted_precision: 0.772\n",
      "weighted_recall: 0.771\n",
      "weighted_f1: 0.771\n"
     ]
    }
   ],
   "source": [
    "# Create a multi-class confusion matrix\n",
    "multiclass_cm = [\n",
    "    [10, 2, 1],  # Class 0 predictions\n",
    "    [3, 15, 2],  # Class 1 predictions\n",
    "    [1, 2, 12]   # Class 2 predictions\n",
    "]\n",
    "\n",
    "# Test per-class metrics\n",
    "print(\"Per-class metrics:\")\n",
    "for i in range(3):\n",
    "    metrics = calculate_per_class_metrics(multiclass_cm, i)\n",
    "    print(f\"\\nClass {i}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.3f}\")\n",
    "\n",
    "# Test macro-average metrics\n",
    "print(\"\\nMacro-averaged metrics:\")\n",
    "macro_metrics = calculate_macro_average(multiclass_cm)\n",
    "for metric_name, value in macro_metrics.items():\n",
    "    print(f\"{metric_name}: {value:.3f}\")\n",
    "\n",
    "# Test weighted-average metrics\n",
    "print(\"\\nWeighted-averaged metrics:\")\n",
    "weighted_metrics = calculate_weighted_average(multiclass_cm)\n",
    "for metric_name, value in weighted_metrics.items():\n",
    "    print(f\"{metric_name}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96cabc",
   "metadata": {},
   "source": [
    "## 3. Comparasion with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7093bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn metrics for comparison\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Convert confusion matrices to true and predicted labels\n",
    "def get_labels_from_confusion_matrix(cm):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            y_true.extend([i] * cm[i][j])\n",
    "            y_pred.extend([j] * cm[i][j])\n",
    "    return np.array(y_true), np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f8da8",
   "metadata": {},
   "source": [
    "## Comparing with sklearn metrics\n",
    "\n",
    "Let's compare our implementation with sklearn's implementation for both binary and multi-class cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79e38b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification Comparison:\n",
      "---------------------------------\n",
      "\n",
      "Our Implementation vs Sklearn:\n",
      "Accuracy:  0.850 vs 0.850\n",
      "Precision: 0.778 vs 0.778\n",
      "Recall:    0.875 vs 0.875\n",
      "F1 Score:  0.824 vs 0.824\n"
     ]
    }
   ],
   "source": [
    "# Binary classification comparison\n",
    "print(\"Binary Classification Comparison:\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Get labels from binary confusion matrix\n",
    "y_true_bin, y_pred_bin = get_labels_from_confusion_matrix(binary_cm)\n",
    "\n",
    "# Our implementation\n",
    "our_metrics = evaluate_binary(binary_cm)\n",
    "\n",
    "# Sklearn metrics\n",
    "sk_accuracy = accuracy_score(y_true_bin, y_pred_bin)\n",
    "sk_precision = precision_score(y_true_bin, y_pred_bin, pos_label=1)\n",
    "sk_recall = recall_score(y_true_bin, y_pred_bin, pos_label=1)\n",
    "sk_f1 = f1_score(y_true_bin, y_pred_bin, pos_label=1)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nOur Implementation vs Sklearn:\")\n",
    "print(f\"Accuracy:  {our_metrics['accuracy']:.3f} vs {sk_accuracy:.3f}\")\n",
    "print(f\"Precision: {our_metrics['precision']:.3f} vs {sk_precision:.3f}\")\n",
    "print(f\"Recall:    {our_metrics['recall']:.3f} vs {sk_recall:.3f}\")\n",
    "print(f\"F1 Score:  {our_metrics['f1_score']:.3f} vs {sk_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd2566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-class Classification Comparison:\n",
      "---------------------------------\n",
      "\n",
      "Macro Averages - Our Implementation vs Sklearn:\n",
      "Precision: 0.768 vs 0.768\n",
      "Recall:    0.773 vs 0.773\n",
      "F1 Score:  0.770 vs 0.770\n",
      "\n",
      "Per-class metrics comparison:\n",
      "\n",
      "Severe:\n",
      "Precision: 0.714 vs 0.714\n",
      "Recall:    0.769 vs 0.769\n",
      "F1 Score:  0.741 vs 0.741\n",
      "\n",
      "Moderate:\n",
      "Precision: 0.789 vs 0.789\n",
      "Recall:    0.750 vs 0.750\n",
      "F1 Score:  0.769 vs 0.769\n",
      "\n",
      "Mild:\n",
      "Precision: 0.800 vs 0.800\n",
      "Recall:    0.800 vs 0.800\n",
      "F1 Score:  0.800 vs 0.800\n"
     ]
    }
   ],
   "source": [
    "# Multi-class classification comparison\n",
    "print(\"\\nMulti-class Classification Comparison:\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Get labels from multi-class confusion matrix\n",
    "y_true_multi, y_pred_multi = get_labels_from_confusion_matrix(multiclass_cm)\n",
    "\n",
    "# Our implementation\n",
    "our_macro = calculate_macro_average(multiclass_cm)\n",
    "\n",
    "# Sklearn metrics\n",
    "sk_precision_macro = precision_score(y_true_multi, y_pred_multi, average='macro')\n",
    "sk_recall_macro = recall_score(y_true_multi, y_pred_multi, average='macro')\n",
    "sk_f1_macro = f1_score(y_true_multi, y_pred_multi, average='macro')\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nMacro Averages - Our Implementation vs Sklearn:\")\n",
    "print(f\"Precision: {our_macro['macro_precision']:.3f} vs {sk_precision_macro:.3f}\")\n",
    "print(f\"Recall:    {our_macro['macro_recall']:.3f} vs {sk_recall_macro:.3f}\")\n",
    "print(f\"F1 Score:  {our_macro['macro_f1']:.3f} vs {sk_f1_macro:.3f}\")\n",
    "\n",
    "# Compare per-class metrics\n",
    "print(\"\\nPer-class metrics comparison:\")\n",
    "for i, severity in enumerate(['Severe', 'Moderate', 'Mild']):\n",
    "    print(f\"\\n{severity}:\")\n",
    "    our_metrics = calculate_per_class_metrics(multiclass_cm, i)\n",
    "    \n",
    "    # Sklearn per-class metrics\n",
    "    sk_precision = precision_score(y_true_multi, y_pred_multi, average=None)[i]\n",
    "    sk_recall = recall_score(y_true_multi, y_pred_multi, average=None)[i]\n",
    "    sk_f1 = f1_score(y_true_multi, y_pred_multi, average=None)[i]\n",
    "    \n",
    "    print(f\"Precision: {our_metrics['precision']:.3f} vs {sk_precision:.3f}\")\n",
    "    print(f\"Recall:    {our_metrics['recall']:.3f} vs {sk_recall:.3f}\")\n",
    "    print(f\"F1 Score:  {our_metrics['f1_score']:.3f} vs {sk_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
